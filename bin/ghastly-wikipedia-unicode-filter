#!/usr/bin/perl -w
#
# ghastly-wikipedia-unicode-filter:
# Absolutely disgusting filter to undo some Unicode madness in the Wikipedia
# XML dump files.
#
# Stop! Stop! It is burning my eyes! The issue here is that the Wikipedia XML
# dumps include surrogate pairs, even though these should only appear in UTF-16
# streams (Wikipedia's XML is UTF-8); further, they occasionally take the
# courageous step of encoding the high half of a surrogate as an &#x....;
# entity. This script strips out all of that crap and emits well-formed UTF-8.
# That's not actually good enough for expat, which still barfs on the replaced
# characters, but using iconv -f utf-8 -t utf-16 to turn it into UTF-16, and
# having expat read it in in that format appears to fix the problem.
#
# Copyright (c) 2005 UK Citizens Online Democracy. All rights reserved.
# Email: chris@mysociety.org; WWW: http://www.mysociety.org/
#

my $rcsid = ''; $rcsid .= '$Id: ghastly-wikipedia-unicode-filter,v 1.3 2005-09-14 23:44:03 chris Exp $';

use strict;
require 5.8.0;

use Text::Iconv;    # would use Encode, but it doesn't support UTF-16
use utf8;
no warnings 'utf8';

binmode(STDIN, ":bytes");
binmode(STDOUT, ":utf8");

sub utf8bytesof ($) {
    my $char = sprintf('%c', $_[0]);
    if ($char eq '<') {
        return "&lt;";
    } elsif ($char eq '>') {
        return "&gt;";
    } elsif ($char eq '&') {
        return "&amp;";
    }
    utf8::encode($char);
    return $char;
}

Text::Iconv->raise_error(1);
my $i = new Text::Iconv('UTF-8', 'UCS-4');
my $j = new Text::Iconv('UCS-4', 'UTF-8');

my $n = 0;
while (defined($_ = <STDIN>)) {
    ++$n;
    # $_ is now bytes
    # convert &#x....; into their UTF-8 byte sequence equivalents
    $_ =~ s/&#x(....);/utf8bytesof(hex($1))/ge;
#    utf8::decode($_);
#    s/[^\x{0000}-\x{8000}]/?/g;

    print $j->convert($i->convert($_));

#    print $_;
}

